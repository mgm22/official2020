{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Mad_GANs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgm22/official2020/blob/master/Mad_GANs_%EC%97%B0%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7oBlAvOk_x_"
      },
      "source": [
        "## MAD GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqEAAHr7k_yG"
      },
      "source": [
        "# Initialization of libraries\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from random import randint\n",
        "from sklearn.mixture import GMM\n",
        "device = torch.device('cuda')\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxvMHh9cljGE",
        "outputId": "84401756-a48f-4188-9b54-ed6a01e9740b"
      },
      "source": [
        "pip install scikit-learn==0.19.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.19.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/c8/8db4108aba5e2166cd2ea4eafa1a4b82f89240a1fa85733029cc2358ad1f/scikit_learn-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 5.8MB/s \n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.19.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxWTooXplqB-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mej_UV1Ik_yH"
      },
      "source": [
        "# defining parameters for the training\n",
        "mb_size = 128 # Batch Size\n",
        "Z_dim = 64  # Length of noise vector\n",
        "X_dim = 1  # Input Length\n",
        "h_dim = 128  # Hidden Dimension\n",
        "lr = 1e-4    # Learning Rate\n",
        "num_gen = 4"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLLu7fllk_yI",
        "outputId": "4ad78769-b99d-4493-e3c1-3aff5f42df54"
      },
      "source": [
        "np.random.seed(1)\n",
        "gmm = GMM(5)\n",
        "gmm.means_ = np.array([[10], [20], [60], [80], [110]])\n",
        "gmm.covars_ = np.array([[3], [3], [2], [2], [1]]) ** 2\n",
        "gmm.weights_ = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "\n",
        "X = gmm.sample(200000)\n",
        "data = X\n",
        "data = (data - X.min())/(X.max()-X.min())\n",
        "#plt.hist(data, 200000, normed=False, histtype='stepfilled', alpha=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class GMM is deprecated; The class GMM is deprecated in 0.18 and will be  removed in 0.20. Use class GaussianMixture instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaqxKjcSk_yJ"
      },
      "source": [
        "G = []\n",
        "for i in range(num_gen):\n",
        "    G.append(torch.nn.Sequential(\n",
        "        torch.nn.Linear(Z_dim, h_dim),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.Linear(h_dim, h_dim),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.Linear(h_dim, X_dim),\n",
        "        torch.nn.Sigmoid()\n",
        "    ).cuda())\n",
        "\n",
        "D = torch.nn.Sequential(\n",
        "    torch.nn.Linear(X_dim, h_dim),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    torch.nn.Linear(h_dim, h_dim),\n",
        "    torch.nn.LeakyReLU(0.2),\n",
        "    torch.nn.Linear(h_dim, num_gen + 1),\n",
        "    torch.nn.Softmax()\n",
        ").cuda()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkaxbJUOk_yJ"
      },
      "source": [
        "G_solver = []\n",
        "for i in range(num_gen):\n",
        "    G_solver.append(optim.Adam(G[i].parameters(), lr))\n",
        "D_solver = optim.Adam(D.parameters(), lr)\n",
        "###\n",
        "loss = nn.CrossEntropyLoss()\n",
        "label_G = Variable(torch.LongTensor(mb_size))\n",
        "label_G = label_G.to(device)\n",
        "label_D = Variable(torch.LongTensor(mb_size))\n",
        "label_D = label_D.to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVO5ki33k_yK"
      },
      "source": [
        "# Reset the gradients to zero\n",
        "params = [G[0], G[1], G[2], G[3], D]\n",
        "def reset_grad():\n",
        "    for net in params:\n",
        "        net.zero_grad()\n",
        "reset_grad()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99agofNTnPql",
        "outputId": "52338d0c-d0a9-4942-843b-ed52561194d6"
      },
      "source": [
        "print(G)\r\n",
        "print(D)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Sequential(\n",
            "  (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (1): PReLU(num_parameters=1)\n",
            "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (3): PReLU(num_parameters=1)\n",
            "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (1): PReLU(num_parameters=1)\n",
            "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (3): PReLU(num_parameters=1)\n",
            "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (1): PReLU(num_parameters=1)\n",
            "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (3): PReLU(num_parameters=1)\n",
            "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (1): PReLU(num_parameters=1)\n",
            "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (3): PReLU(num_parameters=1)\n",
            "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            ")]\n",
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=128, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.2)\n",
            "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (3): LeakyReLU(negative_slope=0.2)\n",
            "  (4): Linear(in_features=128, out_features=5, bias=True)\n",
            "  (5): Softmax(dim=None)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwGwBS9Uk_yK",
        "outputId": "32d5270f-4ce3-4241-d9f6-7d9517e9691e"
      },
      "source": [
        "data_index = 0\n",
        "for it in range(198000):\n",
        "    if ((data_index + 1)*mb_size>len(data)):\n",
        "        data_index = 0\n",
        "\n",
        "    X = torch.from_numpy(np.array(data[data_index*mb_size : (data_index + 1)*mb_size]))\n",
        "    X = X.view(mb_size, 1)\n",
        "    X = X.type(torch.FloatTensor)\n",
        "    X = X.to(device)\n",
        "    Total_D_loss = 0\n",
        "    for i in range(num_gen):\n",
        "        # Dicriminator forward-loss-backward-update\n",
        "        #forward pass\n",
        "        z = torch.FloatTensor(mb_size, Z_dim).uniform_(-1, 1)\n",
        "        z = z.to(device)\n",
        "        G_sample = G[i](z)\n",
        "        D_real = D(X)\n",
        "        D_fake = D(G_sample)\n",
        "        # Calculate the loss\n",
        "        D_loss_real = loss(D_real,label_D.fill_(num_gen + 0.1*randint(-1,1)))\n",
        "        D_loss_fake = loss(D_fake, label_G.fill_(i + 0.1*randint(-1,1)))\n",
        "        D_loss = D_loss_real + D_loss_fake\n",
        "        Total_D_loss = D_loss + Total_D_loss\n",
        "        # Calulate and update gradients of discriminator\n",
        "        D_loss.backward()\n",
        "        D_solver.step()\n",
        "\n",
        "        # reset gradient\n",
        "        reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    \n",
        "    Total_G_loss = 0\n",
        "    for i in range(num_gen):\n",
        "        \n",
        "        z = torch.FloatTensor(mb_size, Z_dim).uniform_(-1, 1)\n",
        "        z = z.to(device)\n",
        "        G_sample = G[i](z)\n",
        "        D_fake = D(G_sample)\n",
        "\n",
        "        G_loss = loss(D_fake, label_D.fill_(num_gen + 0.1*randint(-1,1)))\n",
        "        Total_G_loss = G_loss + Total_G_loss\n",
        "        G_loss.backward()\n",
        "        G_solver[i].step()\n",
        "\n",
        "        # reset gradient\n",
        "        reset_grad()\n",
        "        \n",
        "    data_index = data_index + 1\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, Total_D_loss.data.cpu().numpy(), Total_G_loss.data.cpu().numpy()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter-0; D_loss: 12.877647399902344; G_loss: 6.414348125457764\n",
            "Iter-1000; D_loss: 12.121740341186523; G_loss: 5.436680793762207\n",
            "Iter-2000; D_loss: 12.663895606994629; G_loss: 5.111286640167236\n",
            "Iter-3000; D_loss: 11.78495979309082; G_loss: 6.265835285186768\n",
            "Iter-4000; D_loss: 12.991355895996094; G_loss: 5.523771286010742\n",
            "Iter-5000; D_loss: 11.889249801635742; G_loss: 5.163941383361816\n",
            "Iter-6000; D_loss: 12.41379451751709; G_loss: 5.195889949798584\n",
            "Iter-7000; D_loss: 12.319002151489258; G_loss: 5.25251579284668\n",
            "Iter-8000; D_loss: 12.28233528137207; G_loss: 5.342812538146973\n",
            "Iter-9000; D_loss: 12.237682342529297; G_loss: 5.1449785232543945\n",
            "Iter-10000; D_loss: 11.337226867675781; G_loss: 5.218575477600098\n",
            "Iter-11000; D_loss: 12.183147430419922; G_loss: 5.902035236358643\n",
            "Iter-12000; D_loss: 13.100282669067383; G_loss: 5.2783403396606445\n",
            "Iter-13000; D_loss: 11.565970420837402; G_loss: 5.058351516723633\n",
            "Iter-14000; D_loss: 11.474931716918945; G_loss: 5.139024257659912\n",
            "Iter-15000; D_loss: 12.139199256896973; G_loss: 5.29067325592041\n",
            "Iter-16000; D_loss: 12.34493637084961; G_loss: 5.374022006988525\n",
            "Iter-17000; D_loss: 11.789627075195312; G_loss: 4.9258646965026855\n",
            "Iter-18000; D_loss: 12.91590690612793; G_loss: 5.896725654602051\n",
            "Iter-19000; D_loss: 12.275300025939941; G_loss: 5.044892311096191\n",
            "Iter-20000; D_loss: 12.821493148803711; G_loss: 5.934191703796387\n",
            "Iter-21000; D_loss: 12.711612701416016; G_loss: 4.444350242614746\n",
            "Iter-22000; D_loss: 11.923360824584961; G_loss: 5.7183966636657715\n",
            "Iter-23000; D_loss: 12.438019752502441; G_loss: 5.901632308959961\n",
            "Iter-24000; D_loss: 12.327808380126953; G_loss: 4.855260848999023\n",
            "Iter-25000; D_loss: 11.920576095581055; G_loss: 5.052312850952148\n",
            "Iter-26000; D_loss: 12.471336364746094; G_loss: 4.953200817108154\n",
            "Iter-27000; D_loss: 12.128162384033203; G_loss: 5.439004898071289\n",
            "Iter-28000; D_loss: 12.119474411010742; G_loss: 5.05068302154541\n",
            "Iter-29000; D_loss: 12.555350303649902; G_loss: 5.220493316650391\n",
            "Iter-30000; D_loss: 11.935468673706055; G_loss: 4.740212917327881\n",
            "Iter-31000; D_loss: 12.726295471191406; G_loss: 5.109086990356445\n",
            "Iter-32000; D_loss: 11.769964218139648; G_loss: 5.029942512512207\n",
            "Iter-33000; D_loss: 12.451665878295898; G_loss: 5.231412410736084\n",
            "Iter-34000; D_loss: 12.384020805358887; G_loss: 6.042215347290039\n",
            "Iter-35000; D_loss: 12.090675354003906; G_loss: 6.239491939544678\n",
            "Iter-36000; D_loss: 12.717370986938477; G_loss: 5.893950939178467\n",
            "Iter-37000; D_loss: 12.159008026123047; G_loss: 6.144760608673096\n",
            "Iter-38000; D_loss: 12.665545463562012; G_loss: 5.0937910079956055\n",
            "Iter-39000; D_loss: 12.07669448852539; G_loss: 5.227710723876953\n",
            "Iter-40000; D_loss: 11.858941078186035; G_loss: 5.457427501678467\n",
            "Iter-41000; D_loss: 12.227861404418945; G_loss: 5.051846504211426\n",
            "Iter-42000; D_loss: 12.365114212036133; G_loss: 5.393615245819092\n",
            "Iter-43000; D_loss: 11.799552917480469; G_loss: 5.673435688018799\n",
            "Iter-44000; D_loss: 11.501143455505371; G_loss: 4.929732322692871\n",
            "Iter-45000; D_loss: 12.719462394714355; G_loss: 5.676504135131836\n",
            "Iter-46000; D_loss: 11.733284950256348; G_loss: 5.189017295837402\n",
            "Iter-47000; D_loss: 11.999675750732422; G_loss: 5.022781848907471\n",
            "Iter-48000; D_loss: 11.751047134399414; G_loss: 5.866588592529297\n",
            "Iter-49000; D_loss: 11.76842212677002; G_loss: 5.333560466766357\n",
            "Iter-50000; D_loss: 12.914660453796387; G_loss: 6.1765666007995605\n",
            "Iter-51000; D_loss: 11.282445907592773; G_loss: 5.465500831604004\n",
            "Iter-52000; D_loss: 12.743734359741211; G_loss: 4.979273319244385\n",
            "Iter-53000; D_loss: 12.096940994262695; G_loss: 5.149934768676758\n",
            "Iter-54000; D_loss: 11.958343505859375; G_loss: 6.21641206741333\n",
            "Iter-55000; D_loss: 11.960165023803711; G_loss: 5.711648464202881\n",
            "Iter-56000; D_loss: 12.711899757385254; G_loss: 6.524468421936035\n",
            "Iter-57000; D_loss: 12.100885391235352; G_loss: 5.576332092285156\n",
            "Iter-58000; D_loss: 11.599050521850586; G_loss: 5.502024173736572\n",
            "Iter-59000; D_loss: 12.120113372802734; G_loss: 5.346430778503418\n",
            "Iter-60000; D_loss: 12.368141174316406; G_loss: 6.019073486328125\n",
            "Iter-61000; D_loss: 12.946069717407227; G_loss: 4.941667556762695\n",
            "Iter-62000; D_loss: 12.032491683959961; G_loss: 5.419953346252441\n",
            "Iter-63000; D_loss: 12.789874076843262; G_loss: 4.893556594848633\n",
            "Iter-64000; D_loss: 11.992652893066406; G_loss: 5.5508341789245605\n",
            "Iter-65000; D_loss: 12.639121055603027; G_loss: 5.257715702056885\n",
            "Iter-66000; D_loss: 11.385115623474121; G_loss: 5.596378326416016\n",
            "Iter-67000; D_loss: 12.926797866821289; G_loss: 6.223545074462891\n",
            "Iter-68000; D_loss: 11.809956550598145; G_loss: 4.967349529266357\n",
            "Iter-69000; D_loss: 12.03164005279541; G_loss: 5.972712516784668\n",
            "Iter-70000; D_loss: 11.790040969848633; G_loss: 5.279853820800781\n",
            "Iter-71000; D_loss: 11.968669891357422; G_loss: 5.5928778648376465\n",
            "Iter-72000; D_loss: 12.619926452636719; G_loss: 5.573303699493408\n",
            "Iter-73000; D_loss: 12.405564308166504; G_loss: 4.90250825881958\n",
            "Iter-74000; D_loss: 11.948419570922852; G_loss: 4.898716926574707\n",
            "Iter-75000; D_loss: 12.199506759643555; G_loss: 5.037477493286133\n",
            "Iter-76000; D_loss: 12.480941772460938; G_loss: 4.959264278411865\n",
            "Iter-77000; D_loss: 12.357978820800781; G_loss: 6.0605363845825195\n",
            "Iter-78000; D_loss: 11.554302215576172; G_loss: 5.008461952209473\n",
            "Iter-79000; D_loss: 12.940352439880371; G_loss: 5.630481719970703\n",
            "Iter-80000; D_loss: 12.536296844482422; G_loss: 5.6676411628723145\n",
            "Iter-81000; D_loss: 12.557171821594238; G_loss: 4.909045219421387\n",
            "Iter-82000; D_loss: 12.728754043579102; G_loss: 5.145020961761475\n",
            "Iter-83000; D_loss: 11.35385799407959; G_loss: 6.350176811218262\n",
            "Iter-84000; D_loss: 12.336774826049805; G_loss: 6.2636823654174805\n",
            "Iter-85000; D_loss: 11.800948143005371; G_loss: 4.982572555541992\n",
            "Iter-86000; D_loss: 12.160911560058594; G_loss: 5.765655517578125\n",
            "Iter-87000; D_loss: 12.52798080444336; G_loss: 6.346104621887207\n",
            "Iter-88000; D_loss: 11.389719009399414; G_loss: 5.665718078613281\n",
            "Iter-89000; D_loss: 11.90822982788086; G_loss: 4.933994770050049\n",
            "Iter-90000; D_loss: 11.768755912780762; G_loss: 5.219583511352539\n",
            "Iter-91000; D_loss: 12.088418006896973; G_loss: 5.931962966918945\n",
            "Iter-92000; D_loss: 12.318285942077637; G_loss: 5.563741683959961\n",
            "Iter-93000; D_loss: 12.064370155334473; G_loss: 5.056462287902832\n",
            "Iter-94000; D_loss: 11.303752899169922; G_loss: 5.610842704772949\n",
            "Iter-95000; D_loss: 12.078125; G_loss: 5.708851337432861\n",
            "Iter-96000; D_loss: 12.177311897277832; G_loss: 6.101804733276367\n",
            "Iter-97000; D_loss: 12.393547058105469; G_loss: 4.983048915863037\n",
            "Iter-98000; D_loss: 11.704019546508789; G_loss: 4.917821407318115\n",
            "Iter-99000; D_loss: 11.774710655212402; G_loss: 5.616321086883545\n",
            "Iter-100000; D_loss: 12.12403392791748; G_loss: 5.514911651611328\n",
            "Iter-101000; D_loss: 12.77804946899414; G_loss: 5.838806629180908\n",
            "Iter-102000; D_loss: 13.373769760131836; G_loss: 4.908388137817383\n",
            "Iter-103000; D_loss: 12.652551651000977; G_loss: 5.701781272888184\n",
            "Iter-104000; D_loss: 13.02534008026123; G_loss: 5.920868873596191\n",
            "Iter-105000; D_loss: 12.102091789245605; G_loss: 6.274375915527344\n",
            "Iter-106000; D_loss: 11.906291007995605; G_loss: 5.024653911590576\n",
            "Iter-107000; D_loss: 11.916097640991211; G_loss: 6.299796104431152\n",
            "Iter-108000; D_loss: 13.420076370239258; G_loss: 4.928321838378906\n",
            "Iter-109000; D_loss: 12.143473625183105; G_loss: 4.945156574249268\n",
            "Iter-110000; D_loss: 11.886338233947754; G_loss: 5.157892227172852\n",
            "Iter-111000; D_loss: 12.065164566040039; G_loss: 5.726719856262207\n",
            "Iter-112000; D_loss: 11.423837661743164; G_loss: 4.935987949371338\n",
            "Iter-113000; D_loss: 12.312906265258789; G_loss: 5.605188369750977\n",
            "Iter-114000; D_loss: 12.528550148010254; G_loss: 4.880409240722656\n",
            "Iter-115000; D_loss: 12.149381637573242; G_loss: 5.640860557556152\n",
            "Iter-116000; D_loss: 12.019098281860352; G_loss: 6.311271667480469\n",
            "Iter-117000; D_loss: 11.887275695800781; G_loss: 5.880544662475586\n",
            "Iter-118000; D_loss: 12.799367904663086; G_loss: 5.616396427154541\n",
            "Iter-119000; D_loss: 11.807254791259766; G_loss: 5.619772434234619\n",
            "Iter-120000; D_loss: 11.829778671264648; G_loss: 4.941328048706055\n",
            "Iter-121000; D_loss: 12.29155445098877; G_loss: 5.604401588439941\n",
            "Iter-122000; D_loss: 12.719934463500977; G_loss: 5.687345504760742\n",
            "Iter-123000; D_loss: 12.025498390197754; G_loss: 6.4651336669921875\n",
            "Iter-124000; D_loss: 12.172119140625; G_loss: 5.683885097503662\n",
            "Iter-125000; D_loss: 12.753670692443848; G_loss: 5.6304802894592285\n",
            "Iter-126000; D_loss: 12.560359001159668; G_loss: 5.731462001800537\n",
            "Iter-127000; D_loss: 12.045225143432617; G_loss: 5.869385242462158\n",
            "Iter-128000; D_loss: 12.69990348815918; G_loss: 4.816341876983643\n",
            "Iter-129000; D_loss: 12.316295623779297; G_loss: 4.936107635498047\n",
            "Iter-130000; D_loss: 12.483301162719727; G_loss: 5.672608375549316\n",
            "Iter-131000; D_loss: 12.43000602722168; G_loss: 5.637892723083496\n",
            "Iter-132000; D_loss: 12.699180603027344; G_loss: 4.877331256866455\n",
            "Iter-133000; D_loss: 12.080098152160645; G_loss: 5.24337911605835\n",
            "Iter-134000; D_loss: 13.382564544677734; G_loss: 5.8066487312316895\n",
            "Iter-135000; D_loss: 11.543339729309082; G_loss: 5.639599800109863\n",
            "Iter-136000; D_loss: 11.991781234741211; G_loss: 4.726151466369629\n",
            "Iter-137000; D_loss: 12.750364303588867; G_loss: 6.343967437744141\n",
            "Iter-138000; D_loss: 12.003273963928223; G_loss: 5.523721694946289\n",
            "Iter-139000; D_loss: 12.006189346313477; G_loss: 5.797635078430176\n",
            "Iter-140000; D_loss: 11.765884399414062; G_loss: 5.581056594848633\n",
            "Iter-141000; D_loss: 12.692962646484375; G_loss: 5.041553497314453\n",
            "Iter-142000; D_loss: 12.836443901062012; G_loss: 5.451633453369141\n",
            "Iter-143000; D_loss: 12.36433219909668; G_loss: 5.811058044433594\n",
            "Iter-144000; D_loss: 12.808981895446777; G_loss: 5.717526912689209\n",
            "Iter-145000; D_loss: 11.751680374145508; G_loss: 5.596416473388672\n",
            "Iter-146000; D_loss: 11.381620407104492; G_loss: 5.079018592834473\n",
            "Iter-147000; D_loss: 11.773086547851562; G_loss: 5.675651550292969\n",
            "Iter-148000; D_loss: 12.395994186401367; G_loss: 5.871878623962402\n",
            "Iter-149000; D_loss: 12.127180099487305; G_loss: 4.965790748596191\n",
            "Iter-150000; D_loss: 12.689826011657715; G_loss: 5.474933624267578\n",
            "Iter-151000; D_loss: 12.538249969482422; G_loss: 4.669928073883057\n",
            "Iter-152000; D_loss: 11.618069648742676; G_loss: 5.666162490844727\n",
            "Iter-153000; D_loss: 12.653480529785156; G_loss: 5.23622465133667\n",
            "Iter-154000; D_loss: 12.656682968139648; G_loss: 6.15005350112915\n",
            "Iter-155000; D_loss: 11.45937728881836; G_loss: 5.49221658706665\n",
            "Iter-156000; D_loss: 11.9854154586792; G_loss: 4.992960453033447\n",
            "Iter-157000; D_loss: 12.61679458618164; G_loss: 6.299582481384277\n",
            "Iter-158000; D_loss: 11.54388427734375; G_loss: 4.87693977355957\n",
            "Iter-159000; D_loss: 11.947845458984375; G_loss: 5.654994010925293\n",
            "Iter-160000; D_loss: 12.343536376953125; G_loss: 5.027626037597656\n",
            "Iter-161000; D_loss: 12.293514251708984; G_loss: 5.080385208129883\n",
            "Iter-162000; D_loss: 12.971731185913086; G_loss: 5.04693603515625\n",
            "Iter-163000; D_loss: 11.899154663085938; G_loss: 5.5726847648620605\n",
            "Iter-164000; D_loss: 12.931334495544434; G_loss: 5.1125688552856445\n",
            "Iter-165000; D_loss: 12.73061752319336; G_loss: 4.697997570037842\n",
            "Iter-166000; D_loss: 11.777050971984863; G_loss: 5.677038669586182\n",
            "Iter-167000; D_loss: 12.123455047607422; G_loss: 6.587997913360596\n",
            "Iter-168000; D_loss: 12.540971755981445; G_loss: 5.648711681365967\n",
            "Iter-169000; D_loss: 11.836597442626953; G_loss: 4.960653781890869\n",
            "Iter-170000; D_loss: 11.539484024047852; G_loss: 4.707625389099121\n",
            "Iter-171000; D_loss: 11.94499683380127; G_loss: 5.590194225311279\n",
            "Iter-172000; D_loss: 12.00320053100586; G_loss: 5.617088317871094\n",
            "Iter-173000; D_loss: 11.433324813842773; G_loss: 4.963222980499268\n",
            "Iter-174000; D_loss: 11.801191329956055; G_loss: 4.861891746520996\n",
            "Iter-175000; D_loss: 11.75397777557373; G_loss: 4.732771396636963\n",
            "Iter-176000; D_loss: 12.38998794555664; G_loss: 5.644222259521484\n",
            "Iter-177000; D_loss: 11.7811861038208; G_loss: 5.844613075256348\n",
            "Iter-178000; D_loss: 12.158676147460938; G_loss: 5.8649396896362305\n",
            "Iter-179000; D_loss: 11.926288604736328; G_loss: 5.94529914855957\n",
            "Iter-180000; D_loss: 11.494036674499512; G_loss: 5.520801544189453\n",
            "Iter-181000; D_loss: 12.351879119873047; G_loss: 4.853013038635254\n",
            "Iter-182000; D_loss: 12.227758407592773; G_loss: 5.092588901519775\n",
            "Iter-183000; D_loss: 12.733073234558105; G_loss: 5.94718599319458\n",
            "Iter-184000; D_loss: 12.034550666809082; G_loss: 5.786029815673828\n",
            "Iter-185000; D_loss: 12.751684188842773; G_loss: 5.820441246032715\n",
            "Iter-186000; D_loss: 12.541409492492676; G_loss: 5.681791305541992\n",
            "Iter-187000; D_loss: 11.358863830566406; G_loss: 4.927587509155273\n",
            "Iter-188000; D_loss: 13.511094093322754; G_loss: 5.616672992706299\n",
            "Iter-189000; D_loss: 12.750115394592285; G_loss: 6.038345813751221\n",
            "Iter-190000; D_loss: 12.345693588256836; G_loss: 4.9583330154418945\n",
            "Iter-191000; D_loss: 11.822450637817383; G_loss: 4.937390327453613\n",
            "Iter-192000; D_loss: 11.81618881225586; G_loss: 5.686091899871826\n",
            "Iter-193000; D_loss: 12.39454460144043; G_loss: 5.1152849197387695\n",
            "Iter-194000; D_loss: 12.612154006958008; G_loss: 5.6342387199401855\n",
            "Iter-195000; D_loss: 12.001893997192383; G_loss: 5.652063369750977\n",
            "Iter-196000; D_loss: 12.722495079040527; G_loss: 4.838332176208496\n",
            "Iter-197000; D_loss: 12.784280776977539; G_loss: 5.679338455200195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La_B8q7Bk_yL"
      },
      "source": [
        " Let us see the images generated by the generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0hy45KsNk_yL",
        "outputId": "a8c16891-34a3-4b8b-b1ee-f41dc435ded6"
      },
      "source": [
        "import numpy as np\n",
        "final = np.zeros(1500*mb_size, dtype = float)\n",
        "for i in range(1500):\n",
        "    z = torch.FloatTensor(64, Z_dim).uniform_(-1, 1)\n",
        "    z = z.to(device)\n",
        "    l = G[randint(0,num_gen-1)](z).cpu().detach().numpy()\n",
        "    final[i*mb_size : ((i+ 1)*mb_size -1)] = l[0]\n",
        "p1 = plt.hist(final, 500, alpha = 0.5)\n",
        "p2 = plt.hist(data, 500, alpha = 0.5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}